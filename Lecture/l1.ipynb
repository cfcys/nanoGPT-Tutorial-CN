{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "\n",
    "GPT模型的发展标志着自然语言处理领域的一次重大突破。该模型最初由OpenAI提出，采用了Transformer结构，并以自回归语言模型的形式呈现。GPT-1作为该系列的首个版本，虽然在当时的技术水平下引起了广泛关注，但其规模相对较小，限制了其在处理复杂任务时的表现。然而，这一版本的发布为后续的改进和发展奠定了基础。随后，GPT-2的推出标志着这一系列的进一步演进。GPT-2相较于前作，在模型规模和语料库规模上都有了显著的扩展。这使得模型在语言理解和生成任务上取得了更为优异的表现，同时也吸引了更多研究者和工程师的关注和参与。然而，真正引起轰动的是GPT-3的推出。GPT-3不仅进一步扩大了模型规模，将参数量提升至数十亿级别，还采用了更加庞大和丰富的语料库进行训练。这使得模型在语言理解、文本生成等任务上展现出了前所未有的能力，甚至可以完成一些令人惊讶的语言交互任务，人们开始惊呼：这是智能的涌现....\n",
    "\n",
    "关于后续OpenAi在GPT方面以及Sora的疯狂发展大家有目共睹，但是我们同时沮丧地发现，OpenAI变成了CloseAI,从GPT-3开始的《Language Models are Few-Shot Learners》，OpenAi已经出现了更少阐述模型，更多阐述试验(工程实现)细节的情况，后续的GPT-4和Sora只开放了技术报告更是证明了这一点。\n",
    "\n",
    "此外，我开发此教程还有另一个原因，我在学习nanoGPT时候，年前还能在Bilibili上找到了相关的使用nanoGPT进行中文古诗生成的视频，然后年后去发现相关的视频减少了很多了，截至2024年4月26号，bilibili上能发现的教程只有Andrej Karpathy本人发布的中文翻译视频，以及一个对着代码讲解的，十分生硬的视频。作为一个普通的学生/相关爱好者，我们国内的Ai环境发现有愈来愈封闭的趋势，我们可以学习的途径正在令人恐慌地减少。\n",
    "\n",
    "![image.png](/assets/lecture-pic/GPT-3各版本模型大小.png)\n",
    "\n",
    "从这个图片中可以看出，即使是最小的GPT3 Small 模型，其参数量也是达到了1.25亿，对于没有专业级应用显卡的用户来说，是根本无法去尝试其具体效果的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上多种原因，由Karpathy大神发布的nanoGPT项目成了NLP入门者必学的项目，我将按照其视频思路，致力于更详细地讲解如何去从零构建一个GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
